%sql
CREATE SCHEMA IF NOT EXISTS default1;

---
df = spark.read.option("header", "true").option("inferSchema", "true").csv("dbfs:/FileStore/shashank/raw_data/diagnosis_mapping.csv")
display(df)
df.write.format("delta").mode("append").saveAsTable("default1.raw_diagnosis_map")

---
path1 = "dbfs:/FileStore/shashank/raw_data/patients_daily_file_1_2024.csv"
path2 = "dbfs:/FileStore/shashank/raw_data/patients_daily_file_2_2024.csv"
path3 = "dbfs:/FileStore/shashank/raw_data/patients_daily_file_3_2024.csv"
df1 = spark.read.option("header", "true").option("inferSchema", "true").csv(f"{path3}")
df1 = df1.withColumn("admission_date", df1["admission_date"].cast("date"))
display(df1)
df1.write.format("delta").option("mergeSchema", "true").mode("append").saveAsTable("default1.raw_patients_daily")

---
%sql
TRUNCATE TABLE default1.raw_diagnosis_map;
TRUNCATE TABLE default1.raw_patients_daily;
TRUNCATE TABLE default1.diagnostic_mapping;
TRUNCATE TABLE default1.daily_patients;
TRUNCATE TABLE default1.processed_patient_data;
TRUNCATE TABLE default1.patient_statistics_by_diagnosis;
TRUNCATE TABLE default1.patient_statistics_by_gender;

---
%sql
DROP TABLE default1.raw_diagnosis_map;
DROP TABLE default1.raw_patients_daily;
DROP TABLE default1.diagnostic_mapping;
DROP TABLE default1.daily_patients;
DROP TABLE default1.processed_patient_data;
DROP TABLE default1.patient_statistics_by_diagnosis;
DROP TABLE default1.patient_statistics_by_gender;

---
# Replace <pipeline-id> and <table-name> with your actual pipeline ID and table name
dbutils.fs.rm("dbfs:/pipelines/4cc036a7-f01d-48b4-a55a-5e56257a5d81/checkpoints/daily_patients", True)

---
dbutils.fs.rm("dbfs:/pipelines/4cc036a7-f01d-48b4-a55a-5e56257a5d81/checkpoints/processed_patient_data", True)



